## Im Allgemeinen

**KlassiÔ¨Åkations- und Regressionsmethoden**

- Thema:
  - Vorhersage einer Zielgr√∂√üe (y) durch Pr√§diktoren (Merkmale)
  - Lernen des Zusammenhangs durch Beobachtungen von Zielgr√∂√üe und Pr√§diktoren

- **Regressionsprobleme**:
  - Zielvariable y nimmt kontinuierliche Werte an
  - Beispiel: Vorhersage von Einkommen

- **Klassifikationsprobleme**:
  - Zielvariable y nimmt endliche Werte an ($y \in \{C_1, ..., C_n\}$)
  - Beispiele: Storno / kein Storno, Spam / kein Spam
  - Auspr√§gungen von y werden als Klassen bezeichnet

**Arten von Variablen**

- **Numerische Variablen**:
  - Unterteilung in diskrete und stetige Variablen
  - Diskrete Variablen: Abz√§hlbare Werte (z.B. Anzahl der Kinder)
  - Stetige Variablen:
    - **Intervallskalierte Variablen**: Differenzen haben Bedeutung (z.B. Temperatur in ¬∞C)
    - **Verh√§ltnisskalierte Variablen**: Bedeutsame Quotienten und sinnvoller Nullpunkt (z.B. Herzfrequenz)

- **Kategorielle Variablen**:
  - Unterteilung in ordinalskalierte und nominalskalierte Variablen
  - **Nominalskalierte Variablen**:
    - Keine lineare Ordnung (z.B. Geschlecht: 0/1 oder M/F)
    - Vorsicht bei mehr als zwei Gruppen, um keine falsche Ordinalordnung zu implizieren
  - **Ordinalskalierte Variablen**:
    - Auspr√§gungen k√∂nnen geordnet werden (z.B. Schulnoten)

## Multiple Lineare Regression

- Grundlagen
  - Variablen:
    - $p$ erkl√§rende Variablen $X_1, ..., X_p$
    - Stetige Zielvariable $Y$
  - Ziel:
    - Zusammenhang zwischen $X$ und $Y$ bestimmen: $Y = f(X) + \epsilon$
    - $\epsilon$ repr√§sentiert St√∂rgr√∂√üen (meist additive Annahme)
  - Sch√§tzaufgabe:
    - Beste Sch√§tzung der unbekannten Funktion $f$
    - Trennung zwischen systematischer Komponente $f$ und Fehler $\epsilon$
  - Designmatrix $X$:
    - $X \in R^{n \times (p+1)}$
    - Zeilenvektoren: $x_i$
    - Spalten: $x_1, ..., x_{p+1}$
      $$
      \left(\begin{matrix}
      1&x_{11}&\cdots&x_{1p}\\
      \vdots&\vdots&\ddots&\vdots\\
      1&x_{n1}&\cdots&x_{np}\\
      \end{matrix}\right)
      $$
  - Bedingung:
    - Die Designmatrix $X$ hat vollen Spaltenrang (linear unabh√§ngige Spalten)
  - Generalized Linear Models (GLM):
    - Anwendung in Risikobewertung und √ºberwachten Lernen
    - Typische Vertreter: lineare, logistische, Poisson-Regression
    - Fokus hier: lineares Regressionsmodell

- Lineares Regressionsmodell
  - Modellformulierung:
    - $Y = X\beta + \epsilon$
    - $\epsilon$ repr√§sentiert die Fehlerterme
  - Annahmen:
    - $\mathbb{E}(\epsilon) = 0$ (St√∂rungen im Mittel 0)
    - $Var(\epsilon) = \sigma^2$ (konstante Varianz, Homoskedastizit√§t)
    - $Cov(\epsilon_i, \epsilon_j) = 0$ f√ºr $i \neq j$ (unkorrelierte Fehler)
    - Normalverteilte Fehler: $\epsilon \sim N(0, \sigma^2I)$
  - Eigenschaften:
    - $\mathbb{E}(Y_i) = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}$
    - $Var(Y_i) = \sigma^2$
    - $Cov(Y_i, Y_j) = 0$ f√ºr $i \neq j$
  - Matrixnotation:
    - $\mathbb{E}(Y) = X\beta$
    - $Cov(Y) = \sigma^2 I$
    - $Y \sim N(X\beta, \sigma^2 I)$

- Sch√§tzung im linearen Modell
  - Sch√§tzer der Modellkoeffizienten:
    - $\hat{\beta} = {(X^TX)}^{-1} X^TY$
    - Verteilung: $\hat{\beta} \sim N(\beta, \sigma^2 (X^TX)^{-1})$
  - Sch√§tzer f√ºr die Varianz:
    - $\hat{\sigma}^2 = \frac{1}{n - (p + 1)} \sum_{i=1}^n (y_i - (X\hat{\beta})_i)^2$
    - $(n - (p+1)) \hat{\sigma}^2 \sim \sigma^2 \chi^2_{n-(p+1)}$
    - $\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - (X \hat{\beta})_i)^2 \sim \chi^2_{n-(p+1)}$

- Statistische Tests & Konfidenzintervalle
  - Standardfehler:
    - $se(\hat{\beta}_j) = \hat{\sigma} \sqrt{(X^TX)^{-1}_{jj}}$
  - Hypothesentest f√ºr einzelnen Koeffizienten:
    - Nullhypothese $H_0: \beta_j = 0$, Alternative $H_a: \beta_j \neq 0$
    - Teststatistik: $z_j = \frac{\hat{\beta}_j}{se(\hat{\beta}_j)}$ mit $z_j \sim t(n-(p+1))$
  - Globaler F-Test:
    - Nullhypothese $H_0: \beta_1 = \beta_2 = ... = \beta_p = 0$
    - Alternative: Mindestens ein $\beta_j \neq 0$
    - Keine Einfluss der Kovariablen unter H0: $\hat{\beta}_0 = \bar{y}$
    - Teststatistik: $F = \frac{(TSS - RSS) / p}{RSS / (n - p - 1)}$, $F \sim F_{p, n-p-1}$
  - Konfidenzintervall:
    - $\hat{\beta}_j \pm t_{n-p-1,1-\alpha/2} \cdot se(\hat{\beta}_j)$

- Modellg√ºte & Residuenanalyse
  - Homoskedastizit√§t:
    - Residuen: $\hat{\epsilon} = y - \hat{y} = y - X\hat{\beta}$
    - Residuen sind oft nicht homoskedastisch.
    - Varianz der Residuen: $Var(\hat{\epsilon}_i) = \sigma^2 (1 - h_{ii})$
    - Standardisierte Residuen: $r_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}\sqrt{1-h_{ii}}}$

- Modellerweiterungen
  - Polynomiale Regression:
    - Modell: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$
    - Dient zur Reduktion von Residuen-Trends und Streuung
    - Q-Q-Plot zur Pr√ºfung der Normalverteilung
  - Transformationen:
    - Bei nichtlinearem Zusammenhang, z.B. $\ln(X)$ oder $\sqrt{X}$

## Lineare Diskriminanzanalyse

- $\pi_k$ repr√§sentiert die priori Wahrscheinlichkeit, dass eine Beobachtung zur k-ten Klasse geh√∂rt. Zur Sch√§tzung von $\pi_k$ berechnen wir den Anteil der Beobachtungen im Trainingssatz, die in die k-te Klasse fallen
- $f_k(X) \equiv \Pr(X | Y = k)$ repr√§sentiert die Dichtefunktion von X f√ºr die k-te Klasse. $f_k(x)$ ist gro√ü, wenn eine Beobachtung in der k-ten Klasse wahrscheinlich $X \approx x$ hat, und klein, wenn es unwahrscheinlich ist
- $p_k(x) = \Pr(Y = k | X = x)$ ist die posteriori Wahrscheinlichkeit, dass eine Beobachtung $X = x$ zur k-ten Klasse geh√∂rt
  $\Pr(Y = k | X = x) = \frac{\pi_k f_k(x)}{\sum_{\ell=1}^{K} \pi_\ell f_\ell(x)}$
- Zur Sch√§tzung von $f_k(x)$ m√ºssen wir vereinfachende Annahmen treffen
- Annahme: $X = (X_1, X_2, \ldots, X_p)$ ist multivariat normalverteilt, mit klassenspezifischem Mittelwert und gemeinsamer Kovarianzmatrix
  - $f(\mathbf{x}) = \frac{1}{(2\pi)^\frac{p}{2} \left|\mathbf{\Sigma}\right|^\frac{1}{2}} \exp{\left(-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})\right)}$
  - Voraussetzung: jeder Pr√§diktor folgt einer eindimensionalen Normalverteilung
  - Die Oberfl√§che hat eine Glockenform bei $\mathrm{var}(X_1) = \mathrm{var}(X_2)$ und $\mathrm{cor}(X_1, X_2)=0$
  
```python=
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
LDA(store_covariance=True).fit(X, y)
# X does not have an intercept column
# y is response with two labels (not 0 and 1)
```

## Quadratische Diskriminanzanalyse

- LDA: Beobachtungen aus multivariater Gau√üverteilung, gleiche Kovarianzmatrix f√ºr alle Klassen
- QDA: Beobachtungen aus Gau√üverteilung, jede Klasse eigene Kovarianzmatrix $\Sigma_k$
- Unterschied LDA vs. QDA: Bias-Varianz-Kompromiss.
  - LDA: gemeinsame Kovarianzmatrix, weniger flexibel, niedrigere Varianz, gut bei wenigen Trainingsbeobachtungen
  - QDA: separate Kovarianzmatrizen, h√∂here Varianz, besser bei gro√üen Trainingss√§tzen

```python=
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
QDA(store_covariance=True).fit(X, y)
# X does not have an intercept column
# y is response with two labels (not 0 and 1)
```

## Naive Bayes

- Sch√§tzt $\pi_1, \ldots, \pi_K$ als Anteil der Beobachtungen pro Klasse
- Sch√§tzt $f_1(x), \ldots, f_K(x)$ unter Annahme unabh√§ngiger Pr√§diktoren innerhalb jeder Klasse
  - Quantitativ: $(X_j \mid Y = k) \sim N(\mu_{jk}, \sigma_{jk}^2)$ oder mit nichtparametrischen Methoden (Histogramm, Kernel Density Estimator)
  - Qualitativ: Anteil der Trainingsbeobachtungen f√ºr jede Klasse
- $f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)$
- Vereinfachung durch Unabh√§ngigkeitsannahme: keine Ber√ºcksichtigung der gemeinsamen Verteilung der Pr√§diktoren
- gute Ergebnisse, besonders bei kleinen n im Vergleich zu p

```python=
from sklearn.naive_bayes import GaussianNB
GaussianNB().fit(X, y)
# X does not have an intercept column
# y is response with two labels (not 0 and 1)
```

## Ridge Regression

- Minimiert $\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2$
  - Zwei Kriterien:
    - RSS klein halten
    - Schrumpfungsstrafe $\lambda \sum_{j} \beta_j^2$
  - $\lambda$ kontrolliert die Balance:
    - $\lambda = 0$: Gleich wie Least Squares
    - $\lambda \to \infty$: Sch√§tzungen n√§hern sich null
- Schrumpfungsstrafe:
  - Gilt f√ºr $\beta_1, \ldots, \beta_p$, nicht f√ºr $\beta_0$
  - ${\hat{\beta}}_0 = \bar{y} = \sum_{i=1}^{n} y_i/n$
  - $\beta_0$ misst den Mittelwert bei $x_{i1} = x_{i2} = \ldots = x_{ip} = 0$
- ‚Ñì‚ÇÇ Norm:
  - Die Quadratwurzel der Summe der Quadrate seiner Eintr√§ge von einem Vektor
  - Die ‚Ñì‚ÇÇ-Norm der Ridge-Regressionskoeffizienten: $\sum_{j=1}^{p} \beta_j^2$
- Bias-Variance Trade-off: $\lambda$ erh√∂hen -> Flexibilit√§t und Varianz sinken, Bias steigt
  - Least Squares: Niedriger Bias, hohe Varianz
  - Ridge Regression: Geringere Varianz, etwas h√∂herer Bias
- Gr√ºnde f√ºr die Skalierung:
  - Verlustfunktion = RSS + Regularisierungsterm 
    - Gro√üe y-Skala: Gro√üe RSS, kleinerer Effekt des Regularisierungsterms
    - Kleine y-Skala: Kleine RSS, gr√∂√üerer Effekt des Regularisierungsterms
  - Skalierung des Regularisierungsterms mit der Standardabweichung von y sorgt f√ºr einen konsistenten Effekt auf die Verlustfunktion, unabh√§ngig von der y-Skala

```python=
ridge = skl.Ridge(alpha=0.01)
# skl.ridge_regression(alpha=0.01)
# skl.ElasticNet(alpha=0.01, l1_ratio=0)
scaler = StandardScaler(with_mean=True,  with_std=True)
pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])
pipe.fit(X, Y)
ridge.coef_
```

```python=
skl.ElasticNet.path(X, Y, l1_ratio=0., alphas=lambdas)
# X is standardized design matrix
```

```python=
validation = skm.ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)
ridge = skl.Ridge(alpha=0.01)
results = skm.cross_validate(ridge, X, Y, cv=validation, scoring='neg_mean_squared_error')
-results['test_score']
```

```python=
param_grid = {'ridge__alpha': lambdas}
ridge = skl.ElasticNet(l1_ratio=0)
scaler = StandardScaler(with_mean=True,  with_std=True)
pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])
grid = skm.GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')
grid.fit(X, Y)
best_model = grid.best_estimator_
best_model.named_steps['ridge'].coef_
```

```python=
kfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)
grid = skm.GridSearchCV(pipe, param_grid, cv=kfold, scoring='neg_mean_squared_error')
```

```python=
ridgeCV = skl.ElasticNetCV(alphas=lambdas, l1_ratio=0, cv=kfold)
ridgeCV.fit(Xs, Y)
ridgeCV.coef_
ridgeCV.alpha_
```

```python=
ridgeCV = skl.RidgeCV(alphas=lambdas, store_cv_results = True)
ridgeCV.fit(Xs, Y)
```

- GridSearchCV:
  - Mittelwert und Standardabweichung in jedem Trainingssatz berechnet
- CV:
  - Mittelwert und Standardabweichung im gesamten Datensatz berechnet
- Unterschiede zwischen GridSearchCV und CV wegen Standardisierung

## Lasso Regression

- Minimiert $\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| = \text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|$
- Verwendet ‚Ñì‚ÇÅ-Strafe statt ‚Ñì‚ÇÇ-Strafe
  - ‚Ñì‚ÇÅ-Norm: $\|\beta\|_1 = \sum |\beta_j|$
  - **Vorteil:**
    - Schrumpft Koeffizienten auf null bei gro√üem $\lambda$
    - F√ºhrt zu variablenselektiven Modellen -> sparsame Modelle

```python=
param_grid = {'lasso__alpha': lambdas}
lasso = skl.ElasticNet(l1_ratio=1)
scaler = StandardScaler(with_mean=True,  with_std=True)
pipe = Pipeline(steps=[('scaler', scaler), ('lasso', lasso)])
grid = skm.GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')
grid.fit(X, Y)
```

```python=
lassoCV = skl.ElasticNetCV(alphas=lambdas, l1_ratio=1, cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler), ('lasso', lassoCV)]).fit(X, Y)
```

```python=
lassoCV = skl.LassoCV(n_alphas=100, cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler), ('lasso', lassoCV2)]).fit(X, Y)
```

**Vergleich Ridge, Lasso**

- Lasso:
  - Minimiert $\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2$
  - Einschr√§nkung: $\sum_{j=1}^{p} |\beta_j| \leq s$.
  - Kleinster RSS innerhalb eines Diamanten ($| \beta_1 | + | \beta_2 | \leq s$ bei $p = 2$)
- Ridge:
  - Minimiert $\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2$
  - Einschr√§nkung: $\sum_{j=1}^{p} \beta_j^2 \leq s$.
  - Kleinster RSS innerhalb eines Kreises ($\beta_1^2 + \beta_2^2 \leq s$ bei $p = 2$)
- Best Subset Selection:
  - Minimiert $\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2$
  - Einschr√§nkung: $\sum_{j=1}^{p} I(\beta_j \neq 0) \leq s$
  - Kleinster RSS mit maximal s nicht-null Koeffizienten
- Ellipsen und Einschr√§nkungen:
  - Ellipsen um $\hat{\beta}$ repr√§sentieren RSS-Konturen.
  - Lasso und Ridge Sch√§tzungen sind der Punkt, an dem eine Ellipse die Einschr√§nkung zuerst ber√ºhrt
  - Ridge hat keine scharfen Ecken, daher sind die Koeffizienten nicht null
  - Lasso hat Ecken an den Achsen, daher k√∂nnen einige Koeffizienten null sein
  - p=3: Einschr√§nkung f√ºr Ridge eine Kugel, f√ºr Lasso ein Polyeder
  - p>3: Einschr√§nkung f√ºr Ridge eine Hypersph√§re, f√ºr Lasso ein Polytop
- Einfache Spezialfall:
  - Diagonale Matrix: n = p
  - Least Squares-L√∂sung: $\hat{\beta}_j = y_j$
  - Ridge: $\hat{\beta}^R_j = \frac{y_j}{1 + \lambda}$
    - Ridge schrumpft alle Koeffizienten gleichm√§√üig.
  - Lasso: $\hat{\beta}^L_j = \begin{cases} y_j - \lambda/2 & \text{if } y_j > \lambda/2 \\ y_j + \lambda/2 & \text{if } y_j < -\lambda/2 \\ 0 & \text{if } |y_j| \leq \lambda/2 \end{cases}$
    - Lasso schrumpft Koeffizienten um einen konstanten Betrag $\lambda/2$; kleine Koeffizienten werden zu null
- Bayesianische Interpretation:
  - Ridge: Prior ist eine normale Verteilung
    - Ridge L√∂sung ist der Posterior-Modus bei normaler Prior
  - Lasso: Prior ist eine doppelt-exponentielle Verteilung
    - Lasso L√∂sung ist der Posterior-Modus bei doppelt-exponentieller Prior
    - Gaussian-Prior ist flacher und breiter bei null. Lasso-Prior ist steil bei null, erwartet viele Koeffizienten als null

## Hauptkomponenten

- Hauptkomponenten sind transformierten Pr√§diktoren.
- $Z_1, Z_2, \ldots, Z_M$ sind lineare Kombinationen der originalen Pr√§diktoren $X_1, X_2, \ldots, X_p$
  - $Z_m = \sum_{j=1}^{p} \phi_{jm} X_j$
  - $Var(Z_1)$ ist am gr√∂√üten. $\phi^{'}_{1}\phi_{1}=1$
  - $Var(Z_2)$ ist zweit gr√∂√üten. $\phi^{'}_{2}\phi_{2}=1$ und $\phi^{'}_{2}\phi_{1}=0$
  - $\phi_{j1}, ..., \phi_{jp}$ ist loading Vektor
    - $\Phi=\left(\begin{matrix}\phi_{11}&\ldots&\phi_{m1}\\\vdots&\ddots&\vdots\\\phi_{1p}&\cdots&\phi_{mp}\\\end{matrix}\right)$ 
  - $z_{1m}, ..., z_{nm}$ ist score Vektor
    - $Z=\left(\begin{matrix}z_{11}&\ldots&z_{1m}\\\vdots&\ddots&\vdots\\z_{n1}&\cdots&z_{nm}\\\end{matrix}\right)$
  - $X \approx Z \times \Phi^{'}$
    - $X = Z \times \Phi^{'}$ wenn m = p
- Standardisierung der Pr√§diktoren empfohlen, um gleiche Skala zu gew√§hrleisten.

```python=
scaler = StandardScaler(with_mean=True,  with_std=True)
pca = PCA(n_components=2)
linreg = skl.LinearRegression()
pipe = Pipeline([('scaler', scaler),('pca', pca),('linreg', linreg)])
pipe.fit(X, Y)
```

```python=
scaler = StandardScaler(with_mean=True,  with_std=True)
pca = PCA()
linreg = skl.LinearRegression()
param_grid = {'pca__n_components': range(1, 20)}
pipe_grid = Pipeline([('scaler', scaler),('pca', pca),('linreg', linreg)])
grid = skm.GridSearchCV(pipe_grid, param_grid, cv=kfold, scoring='neg_mean_squared_error')
grid.fit(X, Y)
```

- Der Anteil der erkl√§rten Varianz - Proportion of variance explained (PVE):
  - Nach der Zentrierung der Variablen ist die Gesamtvarianz in einem Datensatz:
    - $\sum_{j=1}^{p} \text{Var}(X_j) = \sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} x_{ij}^2$
  - Die durch die m-te Hauptkomponente erkl√§rte Varianz ist:
    - $\frac{1}{n} \sum_{i=1}^{n} z_{im}^2 = \frac{1}{n} \sum_{i=1}^{n} (\sum_{j=1}^{p} \phi_{jm} x_{ij})^2$
  - Der PVE (Anteil der erkl√§rten Varianz) der m-ten Hauptkomponente ist:
    - $\frac{\sum_{i=1}^{n} z_{im}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2} = \frac{\sum_{i=1}^{n} (\sum_{j=1}^{p} \phi_{jm} x_{ij})^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2}$
  - Um den kumulativen PVE der ersten M Hauptkomponenten zu berechnen:
    - Summiere √ºber jeden der ersten M PVEs
    - Die PVEs aller m√∂glichen Hauptkomponenten summieren sich zu eins
  - Die Varianz der Daten kann zerlegt werden:
    - $\underbrace{\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1} x_{ij}^2}_{\text{Var. der Daten}} = \underbrace{\sum_{m=1}^{M} \frac{1}{n} \sum_{i=1}^{n} z_{im}^2}_{\text{Var. der ersten M HKs}} + \underbrace{\frac{1}{n} \sum_{j=1}^{p} \sum_{i=1}^{n} (x_{ij} - \sum_{m=1}^{M} z_{im} \phi_{jm})^2}_{\text{MSE der M-dimensionalen Approximation}}$
    - Da der erste Term fest ist: Durch Maximierung der Varianz der ersten M Hauptkomponenten minimieren wir den mittleren quadratischen Fehler der M-dimensionalen Approximation und umgekehrt
    - Hauptkomponenten k√∂nnen √§quivalent als Minimierung des Approximationsfehlers oder Maximierung der Varianz betrachtet werden
  - Der PVE entspricht:
    - $1 - \frac{\sum_{j=1}^{p} \sum_{i=1}^{n} ( x_{ij} - \sum_{m=1}^{M} z_{im} \phi_{jm} )^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2} = 1 - \frac{\text{RSS}}{\text{TSS}}$
    - Wir k√∂nnen den PVE als das $R^2$ der Approximation f√ºr X durch die ersten M Hauptkomponenten interpretieren

- Sobald wir die Hauptkomponenten berechnet haben, k√∂nnen wir sie gegeneinander darstellen, um niedrigdimensionale Ansichten der Daten zu erzeugen
- Zum Beispiel k√∂nnen wir darstellen:
  - Den Scorevektor $Z_1$ gegen $Z_2$
  - $Z_1$ gegen $Z_3$
  - $Z_2$ gegen $Z_3$
  - Und so weiter
- Biplot: Darstellung von zwei Hauptkomponentenwerten und den Hauptkomponentenladungen
  - Die Variablen liegen nahe beieinander, wenn sie miteinander korreliert sind

- Algorithmus: Iterativer Algorithmus f√ºr Matrix-Vervollst√§ndigung
  1. Erstelle eine vollst√§ndige Datenmatrix $\tilde{X}$ der Dimension $n \times p$, wobei das $(i, j)$-Element gleich $\tilde{x}_{ij} = \begin{cases} x_{ij} & \text{wenn } (i, j) \in O \\ \bar{x}_j & \text{wenn } (i, j) \notin O, \end{cases}$ ist, wobei $\bar{x}_j$ der Durchschnitt der beobachteten Werte f√ºr die $j$-te Variable in der unvollst√§ndigen Datenmatrix $X$ ist. Hier indiziert $O$ die Beobachtungen, die in $X$ vorhanden sind
  2. Wiederhole die Schritte (a)-(c), bis sich das Ziel (12.14) nicht mehr verringert: 
		- a. L√∂se $\text{minimize}_{A \in \mathbb{R}^{n \times M}, B \in \mathbb{R}^{p \times M}} \{ \sum_{j=1}^{p} \sum_{i=1}^{n} ( \tilde{x}_{ij} - \sum_{m=1}^{M} a_{im} b_{jm} )^2 \}$ (12.13) durch Berechnung der Hauptkomponenten von $\tilde{X}$
		- b. F√ºr jedes Element $(i, j) \notin O$, setze $\tilde{x}_{ij} \leftarrow \sum_{m=1}^{M} \hat{a}_{im} \hat{b}_{jm}$
		- c. Berechne das Ziel $\sum_{(i,j) \in O} ( x_{ij} - \sum_{m=1}^{M} \hat{a}_{im} \hat{b}_{jm} )^2$ (12.14)
  3. Gib die gesch√§tzten fehlenden Eintr√§ge $\tilde{x}_{ij}$, $(i, j) \notin O$ zur√ºck

## Regression der partiellen kleinsten Quadrate

- PCR ist eine un√ºberwachte Methode
- PLS ist eine √ºberwachte Methode. Nutzt Y, um neue Merkmale zu identifizieren, die sowohl die Pr√§diktoren als auch die Antwort gut erkl√§ren
- Bestimmung $Z_1$:
  - Standardisieren der p Pr√§diktoren
  - Setzen jedes $\phi_{j1}$ auf den Koeffizienten der einfachen linearen Regression von Y auf $X_j$. Wir f√ºhren p Regression durch, um p $\phi_{j1}$ zu kakulieren
  - Berechnung von $Z_1 = \sum_{j=1}^{p} \phi_{j1} X_j$ priorisiert Variablen mit der st√§rksten Verbindung zur Antwortvariable
- Berechnung $Z_2$:
  - Regression jedes $X_j$ auf $Z_1$ und Berechnung der Residuen
  - Verwendung dieser Residuen als neuen Datensatz
  - Standardisieren der p Residualwerte
  - Durchf√ºhrung einfacher linearer Regressionen Y auf jede standardisierter Residuen
  - Verwendung der Koeffizienten aus diesen Regressionen als $\phi_{j2}$ Werte: $Z_2 = \sum_{j=1}^{p} \phi_{j2} X_j$
- Wiederhole den Prozess *M* Mal, um $Z_1, ‚Ä¶, Z_M$ zu bestimmen
- Durchf√ºhrung linearer Regressionen Y auf $Z_1, ‚Ä¶, Z_M$. Die Koeffizienten sind $\theta_1, ..., \theta_m$
- $\beta_j = \sum_{m=1}^{M} \theta_m \phi_{jm}$
  - $\beta_j$: Regressionskoeffizient f√ºr den  j-ten Pr√§diktor ($X_j$)
  - $\theta_m$: Regressionskoeffizient f√ºr die m-te PLS-Komponente ($Z_m$)
  - $\phi_{jm}$: Gewicht von $X_j$ auf $Z_m$

```python=
pls = PLSRegression(n_components=2, scale=True)
pls.fit(X, Y)
```

```python=
param_grid = {'n_components':range(1, 20)}
pls = PLSRegression(scale=True)
grid = skm.GridSearchCV(pls, param_grid, cv=kfold, scoring='neg_mean_squared_error')
grid.fit(X, Y)
```

## Spline

**St√ºckweise (piecewise) Polynome**
- Ansatz: separate Polynome niedrigen Grades √ºber verschiedene Regionen von X anpassen 
- Beispiel:
  - Kubisches Polynom: $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$
  - $\beta_0, \ldots, \beta_3$ variieren in verschiedenen X-Bereichen
  - Beispiel mit Knoten bei c:
    - F√ºr $x_i < c$: $y_i = \beta_{01} + \beta_{11} x_i + \beta_{21} x_i^2 + \beta_{31} x_i^3 + \epsilon_i$
    - F√ºr $x_i \geq c$: $y_i = \beta_{02} + \beta_{12} x_i + \beta_{22} x_i^2 + \beta_{32} x_i^3 + \epsilon_i$
- Koeffizienten:
  - Zwei Polynome anpassen: eins f√ºr $x_i < c$, eins f√ºr $x_i \geq c$
  - Sch√§tzung mit Methode der kleinsten Quadrate
- Flexibilit√§t:
  - Mehr Knoten = flexiblere Polynome
  - (K + 1) Polynome f√ºr K Knoten
- Einschr√§nkungen f√ºr kontinuierliche Kurve: 
  - Ein d-Grad Polynom: Kontinuit√§t der Ableitungen bis zum Grad d-1 an jedem Knoten
    - Ein kubisches Polynom: Kontinuit√§t, Kontinuit√§t der ersten und zweiten Ableitung
    - Ein quadratisches Polynom: Kontinuit√§t, Kontinuit√§t der ersten Ableitung
	- Ein lineares Polynom: Kontinuit√§t
	- Eine konstante Linie: 0
- Freiheitsgrade:
  - Ein kubisches Polynom ben√∂tigt 4 Freiheitsgrade
    - Berechnung:
      - 4(K + 1) Freiheitsgrade f√ºr K + 1 kubische Segmente
      - Abzug von 3K Freiheitsgraden
      - Ergebnis: 4(K + 1) - 3K = K + 4
  - Ein quadratisches Polynom ben√∂tigt 3 Freiheitsgrade
    - Ergebnis: 3(K + 1) - 2K = K + 3
  - Eine lineares Polynom ben√∂tigt 2 Freiheitsgrade
    - Ergebnis: 2(K + 1) - K = K + 2
  - Eine konstante Linie ben√∂tigt 1 Freiheitsgrade
    - Ergebnis: K + 1

**Basis-Spline**

- Kubischer Spline mit K Knoten:
  - $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 h(x, \xi_1) + \ldots + \beta_{K+3} h(x, \xi_K) + \epsilon_i$
  - Truncated Power Basisfunktion pro Knoten:
    - $h(x, \xi) = (x - \xi)_+^3 = \begin{cases} (x - \xi)^3 & \text{wenn } x > \xi \\ 0 & \text{sonst} \end{cases}$
  - Es gibt 4 + K Koeffizienten -> 4 + K Freiheitsgrade

```python=
from ISLP.transform import BSpline
ageBSDF = BSpline(internal_knots=[25,40,60], intercept=True).fit_transform(WageDF.age)
sm.OLS(WageDF.wage, ageBSDF).fit()
```

**Nat√ºrlicher Spline**

- Basis-Splines haben hohe Varianz an den R√§ndern der Pr√§diktoren, wenn X sehr klein oder sehr gro√ü ist.
- Ein nat√ºrlicher Spline hat zus√§tzliche Randbedingungen: Funktion ist linear am Rand.
  - Ergebnis: 4 + K - 2 = 2 + K

```python=
from ISLP.transform import NaturalSpline
ageNSDF = NaturalSpline(internal_knots=[25,40,60], intercept=True).fit_transform(WageDF.age)
sm.OLS(WageDF.wage, ageNSDF).fit()
```

**Gl√§ttungsspline (Smoothing Spline)**

- Minimierung von $\sum_{i=1}^{n} (y_i - g(x_i))^2 + \lambda \int (g''(t))^2 \, dt$
  - $\lambda \geq 0$: Tuning-Parameter zur Kontrolle der Gl√§tte
    - $\lambda = 0$: Die Funktion interpoliert die Daten exakt
    - $\lambda \to \infty$: Die Funktion ist eine gerade Linie
    - $\lambda$ steuert den Bias-Varianz-Kompromiss. Je gr√∂√üer der Wert von $\lambda$, desto glatter wird $g$
  - $\sum_{i=1}^{n} (y_i - g(x_i))^2$ ist eine Verlustfunktion. Sie sorgt daf√ºr, dass $g$ gut an die Daten angepasst wird
  - $\lambda \int (g''(t))^2 dt$ bestraft die Variabilit√§t von $g$
    - $g''(t)$ misst, wie stark sich die Steigung von $g$ an der Stelle $t$ ver√§ndert
    - Gro√üer absoluter Wert: $g(t)$ ist in der N√§he von $t$ sehr "wellig"
    - Nahe Null: $g(t)$ ist an dieser Stelle glatt (zum Beispiel eine Gerade, deren zweite Ableitung null ist)
    - $\int (g''(t))^2 dt$ misst die gesamte √Ñnderung von $g'(t)$ (der ersten Ableitung bzw. Steigung) √ºber den gesamten Bereich
- $g(x)$ ist nicht identisch mit dem nat√ºrlichen kubischen Spline aus dem Basisfunktionsansatz mit Knoten bei $x_1, \ldots, x_n$.
  - $g(x)$ ist eine geschrumpfte Version eines solchen nat√ºrlichen kubischen Splines.
  - Der Grad der Schrumpfung wird durch $\lambda$ bestimmt.
- Nominale Freiheitsgrade z√§hlen die Anzahl der freien Parameter (z.B. die Anzahl der Koeffizienten in einem Polynom oder Spline)
  - F√ºr Gl√§ttungssplines: $n$ Parameter $\Rightarrow n$ nominale Freiheitsgrade
  - Diese $n$ Parameter werden durch $\lambda$ stark eingeschr√§nkt (geschrumpft). Daher ist die tats√§chliche Flexibilit√§t kleiner als $n$.
- $df_\lambda$ sind die effektiven Freiheitsgrade und messen die Flexibilit√§t des Gl√§ttungssplines
  - H√∂here $df_\lambda$: flexibler (niedrigere Verzerrung, h√∂here Varianz)
  - Niedrigere $df_\lambda$: glatter (h√∂here Verzerrung, niedrigere Varianz)
  - $\hat{g}_\lambda = S_\lambda y$
    - $\hat{g}_\lambda$: Der Vektor der gesch√§tzten Werte des Gl√§ttungssplines f√ºr ein gegebenes $\lambda$
    - $S_\lambda$: Eine $n\times n$-Matrix ("Smoother-Matrix"), die auf den Antwortvektor $y$ angewendet wird
    - $df_\lambda$ ist die Summe der Diagonaleintr√§ge von $S_\lambda$
- Leave-One-Out-Cross-Validation kann f√ºr Gl√§ttungssplines effizient berechnet werden
  - $RSS_{cv}(\lambda) = \sum_{i=1}^{n} (y_i - \hat{g}_{\lambda}^{(-i)}(x_i))^2 = \sum_{i=1}^{n} \left[ \frac{y_i - \hat{g}_{\lambda}(x_i)}{1 - \{S_{\lambda}\}_{ii}} \right]^2$

```python=
from pygam import (s as s_gam, l as l_gam, f as f_gam, LinearGAM)
smooth_spline = LinearGAM(s_gam(0, lam=0.6))
ageCol = np.asarray(WageDF.age).reshape((-1,1))
smooth_spline.fit(ageCol, WageDF.wage)
# s_gam() take only one integer, not list. 
# If multiple columns are needed, use sperate s_gam() for each column.
```

**GAM**

```python=
XSmS_GamARR = np.column_stack( [WageDF.age, WageDF.year, WageDF.education.cat.codes] )
GAM = LinearGAM(s_gam(0) + l_gam(1, lam=0) + f_gam(2, lam=0))
GAM.fit(XSmS_GamARR, WageDF.wage)
```

## Lokales Regressionsmodell

- Lokale Regression ist ein alternativer Ansatz zur Anpassung flexibler, nichtlinearer Funktionen.
- Die Anpassung an einem Zielpunkt $x_0$ wird nur mit nahegelegenen Trainingsdaten berechnet.
- Algorithmus: Lokale Regression bei $X=x_0$
  1. Auswahl der Nachbarschaft
      - Bestimme den Anteil $s=k/n$ der Trainingspunkte, deren $x_i$ am n√§chsten zu $x_0$ liegen.
  2. Gewichtszuteilung
      - Weisen Sie jedem Punkt in dieser Nachbarschaft ein Gewicht $K_{i0}=K(x_i,x_0)$ zu,
          - Der am weitesten entfernte Punkt von $x_0$: Gewicht null
          - Der n√§chstgelegene Punkt: h√∂chstes Gewicht
          - Alle anderen Punkte (nicht unter den $k$-n√§chsten Nachbarn): Gewicht null
  3. Gewichtete Regression
      - Sch√§tze eine gewichtete kleinste-Quadrate-Regression von $y_i$ auf $x_i$ unter Verwendung dieser Gewichte.
      - Finde $\hat{\beta}_0$, $\hat{\beta}_1$, die $\sum_{i=1}^n K_{i0}(y_i-\beta_0-\beta_1 x_i)^2$ minimieren.
  4. Vorhersage
      - Der gesch√§tzte Wert bei $x_0$ ist $\hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1 x_0$
- Die Gewichte $K_{i0}$ sind f√ºr jedes $x_0$ einzigartig.
      - F√ºr jeden Zielpunkt muss eine neue gewichtete Regression berechnet werden.
- Speicherbasierte Methode
    - √Ñhnlich wie das k-n√§chste-Nachbarn-Verfahren
    - Ben√∂tigt die gesamten Trainingsdaten f√ºr jede Vorhersage
- Der Span $s$ ist der Anteil der Punkte, die f√ºr die lokale Anpassung bei $x_0$ verwendet werden.
    - Kleineres $s$: st√§rker lokalisiert und "wackeliger" Fit
    - Gr√∂√üeres $s$: globalerer Fit unter Verwendung mehr Trainingsdaten
- Verallgemeinerungen der lokalen Regression
    - Global in manchen Variablen
    - Lokal in anderen
    - Lokale Regression in zwei Variablen
        - Sch√§tze bivariate Regressionsmodelle nahe jedem Zielpunkt im 2D-Raum
        - Erweiterbar auf h√∂here Dimensionen
        - Die Leistung verschlechtert sich, wenn $p>3$ oder $4$, da dann zu wenige Trainingsdaten in der N√§he sind

```python=
import statsmodels.api as sm
lowess = sm.nonparametric.lowess
lowess(WageDF.wage, WageDF.age, frac=0.2)
```

## Entscheidungsb√§ume

**Regressionsb√§ume**

- Aufteilung des Pr√§diktorraums
  - Grundprinzip
    - Pr√§diktorraum ($X_1, X_2, \ldots, X_p$) wird in $J$ Regionen ($R_1, R_2, \ldots, R_J$) unterteilt.
    - In jeder Region $R_j$ erfolgt die gleiche Vorhersage: Mittelwert der Antwortwerte der Trainingsdaten in $R_j$.

  - Beispiel
    - Zwei Regionen $R_1$ und $R_2$:
      - Mittelwert in $R_1$: 10
      - Mittelwert in $R_2$: 20
    - Vorhersage f√ºr $Y$:
      - $Y \in R_1$: Vorhersage 10
      - $Y \in R_2$: Vorhersage 20

  - Konstruktion der Regionen
    - Regionen als hochdimensionale Rechtecke (Boxen) definiert
    - Minimierung der RSS: $\sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$

  - Ansatz: Rekursive bin√§re Teilung (top-down, gierig)
    - Wahl $X_j$ und Schnittpunkt $s$ zur Aufteilung in $\{ X| X_j < s \}$ und $\{ X| X_j \geq s \}$
    - Suche nach $(j, s)$, die den RSS minimieren: $\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2$
      
  - Rekursive Anwendung und Stoppkriterium
    - W√§hle besten Pr√§diktor und Schnittpunkt jeweils erneut f√ºr entstehende Regionen
    - Wiederhole den Prozess, bis ein Abbruchkriterium erreicht ist, z.B. maximal 5 Beobachtungen pro Region

- √úberanpassung (Overfitting)
  - Gro√üe B√§ume passen sich zu gut an Trainingsdaten an, aber sie generalisieren schlecht auf Testdaten
  - L√∂sung: Kleinere B√§ume
    - Kleinere B√§ume reduzieren die Varianz und sind besser interpretierbar
    - Nachteil: Risiko, wichtige Strukturen zu √ºbersehen

- Pruning (Beschneiden) von B√§umen
  - Zuerst einen gro√üen Baum $T_0$ wachsen lassen
  - Baum dann beschneiden (Pruning), um einen optimalen Subbaum zu erhalten

  - Kostenkomplexit√§ts-Pruning (Weakest Link Pruning)
    - Statt alle Teilb√§ume einzeln zu pr√ºfen, betrachtet man eine durch den nicht-negativen Parameter $\alpha$ indizierte Folge von Teilb√§umen
    - F√ºr jeden Wert von $\alpha$ wird ein Teilbaum $T \subset T_0$ ausgew√§hlt, der folgendes minimiert: $\sum_{m=1}^{|T|} \sum_{i: x_i \in R_m } (y_i - \hat{y}_{R_m})^2 + \alpha |T|$ minimiert
      - $|T|$: Anzahl der terminalen Knoten (Bl√§tter)
    - $\alpha$ steuert Gleichgewicht zwischen Baumkomplexit√§t und Anpassung
      - $\alpha = 0$: keine Bestrafung, $T = T_0$
      - Mit steigendem $\alpha$ werden kleinere, einfachere Subb√§ume bevorzugt

  - Ziel: Testfehler minimieren
    - Subbaum mit geringstem Testfehler ausw√§hlen
    - Testfehler wird mittels Kreuzvalidierung oder Validierungsdaten gesch√§tzt

- Algorithmus: Vorgehen beim Regressionsbaum
  1. Gro√üen Baum wachsen lassen
      - Rekursive bin√§re Teilung durchf√ºhren
      - Stoppen, wenn ein Knoten weniger als die Mindestanzahl Beobachtungen enth√§lt
  2. Kostenkomplexit√§ts-Pruning anwenden
      - Sequenz der besten Subb√§ume f√ºr verschiedene $\alpha$ berechnen
  3. Kreuzvalidierung zur Auswahl von $\alpha$
      - Daten in $K$ Folds teilen
      - F√ºr jedes $\alpha$: Baum auf $K-1$ Folds bauen, Fehler auf dem Auslass-Fold messen
      - Das $\alpha$ w√§hlen, das den mittleren Vorhersagefehler minimiert

  4. Der Teilbaum wird ausgew√§hlt (aus der Folge von Teilb√§umen), der zum optimalen $\alpha$ geh√∂rt.

  - Erl√§uterung Schritt 2
      - Nach dem vollst√§ndigen Baum wird das Fehlerma√ü MSE berechnet
      - Ein Knoten wird geschnitten, das MSE des beschnittenen Baums und die Anzahl der entfernten Bl√§tter werden berechnet
      - Das zugeh√∂rige $\alpha$ f√ºr diesen Schnitt ist die Differenz der Fehlerma√üe geteilt durch die Zahl der entfernten Bl√§tter

```python=
from sklearn.tree import (DecisionTreeRegressor as DTR, plot_tree, export_text)
import sklearn.model_selection as skm
Regression_TRE = DTR()
# max_depth
DTR(XARR_train, y_train)
ccp_path = Regression_TRE.cost_complexity_pruning_path(XARR_train, y_train)
kfold = skm.KFold(5, shuffle=True, random_state=10)
grid = skm.GridSearchCV(Regression_TRE, {'ccp_alpha': ccp_path.ccp_alphas}, refit=True, cv=kfold, scoring='neg_mean_squared_error').fit(XARR_train, y_train)
best_ = grid.best_estimator_
ax = plt.subplots(figsize=(12,12))[1]
plot_tree(best_, feature_names=feature_names, ax=ax)
print(export_text(best_, feature_names=feature_names))
```

**Klassifikationsb√§ume**

- Klassifikationsbaum vs. Regressionsbaum:
  
  - Regressionsbaum: Vorhersage durch Mittelwert der Trainingsbeobachtungen.
  - Klassifikationsbaum: Vorhersage durch h√§ufigste Klasse der Trainingsbeobachtungen.

- Klassifikationsbaum erstellen:
  
  - Nutzen rekursives bin√§res Teilen.
- Klassifikationsfehlerrate als Kriterium:
  - Fehlerquote: $E = 1 - \max_k {\hat{p}}_{mk}$.
    - $k$ ist eine Klasse, $m$ ist ein Bereich im Klassifikationsbaum.
    - ${\hat{p}}_{mk}$ ist der Anteil der Trainingsbeobachtungen im Bereich $m$ aus der Klasse $k$.
    - Beispiele:
      - $k = 1$ k√∂nnte "Herzkrankheit" sein, $k = 2$ "Keine Herzkrankheit".
      - $m = 3$ bezieht sich auf den dritten Endknoten.
      - ${\hat{p}}_{3,1}$ ist der Anteil der Beobachtungen im dritten Bereich, die als "Herzkrankheit" klassifiziert sind.

- Bessere Kriterien als Klassifikationsfehlerrate:
  
  - Gini-Index: $G = \sum_{k=1}^{K} {\hat{p}}_{mk} (1 - {\hat{p}}_{mk})$
    - Klein, wenn Knoten rein ist.
  - Entropie: $D = -\sum_{k=1}^{K} {\hat{p}}_{mk} \log {\hat{p}}_{mk}$
    - Klein bei reinen Knoten.

- Node Purity:
  
  - Split erh√∂ht Knotenreinheit.
  - Wichtig f√ºr genaue Vorhersagen.
  - Beispiel: Split "RestECG<1" am Baumende erh√∂ht Reinheit, obwohl Vorhersage gleich bleibt.

```python=
from sklearn.tree import (DecisionTreeClassifier as DTC, plot_tree, export_text)
from sklearn.metrics import accuracy_score
import sklearn.model_selection as skm
Clas_TRE = DTC(criterion='entropy')
Clas_TRE.fit(XARR_train, label_train)
ccp_path = TRE_clas.cost_complexity_pruning_path(XARR_train, label_train)
kfold = skm.KFold(10, random_state=1, shuffle=True)
grid = skm.GridSearchCV(Clas_TRE, {'ccp_alpha': ccp_path.ccp_alphas}, refit=True, cv=kfold, scoring='accuracy').fit(XARR_train, label_train)
best_ = grid.best_estimator_
grid.best_score_ # Training sample
accuracy_score(label_test, best_.predict(X_test)) # Test sample
grid.best_estimator_.tree_.n_leaves
ax = plt.subplots(figsize=(12,12))[1]
plot_tree(best_, feature_names=feature_names, ax=ax)
print(export_text(best_, feature_names=feature_names, show_weights=True))
```

**Bagging (Bootstrap Aggregation)**

- Bootstrap: Grundlagen und Nutzen

  - Einf√ºhrung
    - Bootstrap ist eine m√§chtige Methode in der Statistik.
    - Wird verwendet, um Standardabweichungen zu berechnen, wenn dies auf direktem Wege schwierig ist
    - Kann zur Verbesserung statistischer Lernmethoden wie Entscheidungsb√§ume verwendet werden

  - Entscheidungsb√§ume und Varianz
    - Entscheidungsb√§ume sind durch hohe Varianz gekennzeichnet
      - Unterschiedliche Trainingsdaten f√ºhren zu stark abweichenden Ergebnissen
    - Methoden mit geringer Varianz (z.‚ÄØB. lineare Regression) liefern stabilere Ergebnisse bei neuen Datens√§tzen
    - Bagging reduziert die Varianz eines Modells
      - Viele Trainingsdatens√§tze erzeugen, Modelle darauf trainieren, Vorhersagen mitteln
      - In der Praxis durch Bootstrapping umgesetzt

- Bagging: Vorgehensweise und Anwendung

  - Allgemeine Vorgehensweise
    - Erstellen von B bootstrapped Trainingss√§tzen durch Ziehen mit Zur√ºcklegen aus dem urspr√ºnglichen Datensatz
    - Trainieren eines Modells (z.‚ÄØB. Entscheidungsbaum) auf jedem bootstrapped Datensatz
    - Mitteln (bei Regression) oder Mehrheitswahl (bei Klassifikation) der Vorhersagen √ºber alle Modelle

- Out-of-Bag (OOB) Fehlerabsch√§tzung

  - Prinzip
    - Erm√∂glicht eine Sch√§tzung des Testfehlers ohne separate Kreuzvalidierung
    - Jeder Baum nutzt im Mittel etwa zwei Drittel der Daten zum Training
    - Das verbleibende Drittel sind Out-of-Bag (OOB) Beobachtungen

  - Vorgehen
    - F√ºr jede OOB-Beobachtung erfolgt die Vorhersage nur durch die Modelle, die diese Beobachtung nicht zum Training genutzt haben
    - OOB-Vorhersagen werden gemittelt (Regression) oder per Mehrheitswahl (Klassifikation) aggregiert
    - Der OOB-Fehler entspricht typischerweise dem Fehler einer Leave-One-Out Kreuzvalidierung

- Variable Importance Measures (Variablenwichtigkeit)

  - Auswirkungen des Bagging auf Interpretierbarkeit
    - Bagging erh√∂ht die Vorhersagegenauigkeit durch Kombination vieler Modelle
    - Die Interpretierbarkeit ist im Vergleich zu einem einzelnen Baum reduziert

  - Messung der Variablenwichtigkeit
    - Bei Regressionsb√§umen: Bedeutung eines Pr√§diktors durch durchschnittliche Reduktion des RSS √ºber alle B√§ume
    - Bei Klassifikationsb√§umen: Bedeutung eines Pr√§diktors durch die durchschnittliche Reduktion des Gini-Index

  - Darstellung
    - Grafische Darstellungen zeigen die wichtigsten Pr√§diktoren und ihre relative Bedeutung

- √úberanpassung
  - Keine √úberanpassungsgefahr bei Erh√∂hung der Baumanzahl in Bagging
  - Zu wenige B√§ume k√∂nnen jedoch zu Unteranpassung f√ºhren

```python=
from sklearn.ensemble import RandomForestRegressor as RF
BAGG = RF(max_features=len(X_train.columns), n_estimators=500, random_state=0).fit(X_train, y_train)
# n_estimators is B
np.mean( (BAGG.predict(X_test) - y_test)**2)
BAGG.feature_importances_
```

**Random Forest**

- Dekorrelierung der B√§ume
  - Bei jedem Split wird eine zuf√§llige Auswahl von m Pr√§diktoren betrachtet (statt alle)
  - Dadurch werden die Entscheidungsb√§ume st√§rker voneinander unterschieden ("dekorreliert")
  - Typischerweise wird $m \approx \sqrt{p}$ gew√§hlt
  - Starke Pr√§diktoren sollen nicht jeden Split dominieren
- Vorgehen/Verfahren (Ablauf des Random Forest)
  - Entscheidungsb√§ume werden auf bootstrapped Trainingsproben trainiert
  - An jedem Knoten wird aus den m zuf√§llig ausgew√§hlten Pr√§diktoren der beste Split gew√§hlt
- Vorteile
  - Geringere Korrelation zwischen den B√§umen
  - Reduzierte Varianz und robustere Vorhersagen
- √úberanpassung
  - Random Forests √ºberpassen nicht, wenn die Anzahl der B√§ume B erh√∂ht wird
  - B sollte so gro√ü gew√§hlt werden, dass sich der Fehler stabilisiert

**Boosting**

- Allgemeiner Ansatz f√ºr Regression und Klassifikation. Hier auf Entscheidungsb√§ume beschr√§nkt
- Unterschied zu Bagging:
  - B√§ume werden sequentiell anstatt unabh√§ngig aufgebaut
  - Keine Bootstrap-Stichproben, sondern modifizierte Datens√§tze
- Algorithmus f√ºr Regression:
  1. Setze ${\hat{f}}{(x)} = 0$ und Residuen $r_i = y_i$ f√ºr alle i im Trainingssatz
  2. F√ºr b = 1, 2, ..., B:
     - Passe einen Baum ${\hat{f^b}}$ mit $d$ Schnitt ($d+1$ Bl√§tter) an die Daten $(X, r)$ an
     - Aktualisiere Modell: ${\hat{f}}(x) \leftarrow {\hat{f}}(x) + Œª {\hat{f^{b}}}{(x)}$
     - Aktualisiere Residuen: $r_i \leftarrow r_i - Œª {\hat{f^{b}}}{(x_i)}$
  3. Ausgabe des Modells: ${\hat{f}}{(x)} = \sum_{b=1}^B {Œª {\hat{f^{b}}}{(x)}}$
  - Erkl√§rung Schritt 2
    - Einen Regressionsbaum ${\hat{f^b}}$ mit maximal d Teilungen auf die Residuen r anpassen (begrenzte Tiefe verhindert √úberanpassung)
    - Gesamtvorhersage ${\hat{f}}$ mit $\lambda$-facher Vorhersage des neuen Baums aktualisieren (Œª steuert den Einfluss).
    - Residuen aktualisieren, indem die Beitr√§ge des neuen Baums (mit Œª gewichtet) abgezogen werden ‚Äì folgende B√§ume konzentrieren sich so auf verbleibende Fehler
- Idee:
  - Jeder Baum wird auf die Residuen (Fehler) des aktuellen Modells angepasst, nicht auf die Originalwerte
  - Die B√§ume sind meist klein (wenige Endknoten, gesteuert durch $d$)
  - Der Schrumpfungsparameter $\lambda$ reguliert das Lerntempo und verhindert √úberanpassung
  - Das Modell wird gezielt dort verbessert, wo es noch Fehler macht
  - Der Aufbau jedes Baums h√§ngt von den zuvor gebauten B√§umen ab
- Abstimmungsparameter:
  1. Anzahl der B√§ume $B$:
     - Gro√üe Anzahl $B$ kann zu √ºberanpassen f√ºhren
     - Kreuzvalidierung zur Auswahl von $B$
  2. Shrinkage-Parameter $\lambda$:
     - Kontrolliert die Lernrate
     - Typische Werte: 0.01 oder 0.001
  3. Anzahl $d$ der Splits pro Baum:
     - Kontrolliert die Komplexit√§t
     - Oft funktioniert d = 1 gut

```python=
from sklearn.ensemble import GradientBoostingRegressor as GBR
# GradientBoostingClassifier as GBC
Boost = GBR(n_estimators=5000, learning_rate=0.001, max_depth=3, random_state=0).fit(X_train, y_train)
np.mean( (y_test - Boost.predict(X_test))**2)
```

**BART (Bayesian Additive Regression Trees)**

- √úberblick
    - BART ist ein Ensemble-Verfahren mit Entscheidungsb√§umen als Basis, meist f√ºr Regression.
    - Verwandt mit Bagging/Random Forests (zuf√§llige B√§ume) und Boosting (Anpassung an Residuen).
        - Kombiniert beide Ans√§tze: zuf√§llige B√§ume, Anpassung an Residuen, B√§ume werden leicht ver√§ndert statt neu gebaut.
- Notation
    - $K$: Anzahl der B√§ume
    - $B$: Anzahl der Iterationen
    - ${\hat{f}}^b_k(x)$: Vorhersage des $k$-ten Baums in Iteration $b$
    - ${\hat{f}}^b(x) = \sum_{k=1}^K {\hat{f}}^b_k(x)$: Modellvorhersage in Iteration $b$
- Algorithmus
    1. Initialisierung
        - Jeder Baum sagt Mittelwert der Antwort geteilt durch $K$ vorher: ${\hat{f}}^1_k(x) = \frac{1}{nK}\sum_{i=1}^n y_i$
        - Anfangsmodell ist der Mittelwert der Antworten ${\hat{f}}^1(x) = \sum_{k=1}^K {{\hat{f}}^1_k(x)} = \frac{1}{n} \sum_{i=1}^n y_i$
    2. Iterative Updates ($b = 2, ..., B$)
        - F√ºr jeden Baum $k$:
            - Berechne partielle Residuen: ziehe aktuelle Vorhersagen der anderen B√§ume ab, au√üer vom Baum $k$: $r_i = y_i - \sum_{k^\prime < k}{{\hat{f}}_{k'}^b(x_i)} - \sum_{k^\prime > k}{{\hat{f}}_{k'}^{b-1}(x_i)}$
            	- $\sum_{k^\prime < k}{{\hat{f}}_{k'}^b(x_i)}$: Summe der Vorhersagen der B√§ume in der *aktuellen Iteration ($b$)*, die bereits aktualisiert wurden ($k'$ kleiner als $k$)
            	- $\sum_{k^\prime > k}{{\hat{f}}_{k'}^{b-1}(x_i)}$: Summe der Vorhersagen der B√§ume aus der *vorherigen Iteration ($b-1$)*, die in der aktuellen Iteration noch nicht aktualisiert wurden ($k'$ gr√∂√üer als $k$)
            - Aktualisiere Baum $k$ zuf√§llig (wachsen, stutzen, Endknoten anpassen); Ziel ist bessere Anpassung
        - Modell ist die Summe der $K$ aktualisierten B√§ume: ${\hat{f}}^b(x) = \sum_{k=1}^K {\hat{f}}^b_k(x)$
    - Finale Vorhersage: Mittelwert der Vorhersagen aus den Iterationen $L+1$ bis $B$: $\hat{f}(x) = \frac{1}{B-L} \sum_{b=L+1}^B {\hat{f}}^b(x)$
    - Baumver√§nderungen:
        - Grow: Teilung hinzuf√ºgen
        - Prune: Teilung entfernen
        - Endknoten anpassen
        - √Ñnderungen sind zuf√§llig, bevorzugen Fehlerreduktion, balancieren Passung und Regularisierung, sind als Posterior-Sampling interpretierbar
- Performance und √úberanpassung
    - Nach dem Burn-in stabilisieren sich Fehlerraten; wenige √úberanpassung
    - Bessere Kontrolle der Modellkomplexit√§t als Boosting
- Bayes-Perspektive
    - Updates sind als Ziehen aus der Posterior-Verteilung interpretierbar (MCMC f√ºr Baum-Ensembles)
- BART-Tuning
    - Zu w√§hlen sind: Anzahl der B√§ume $K$ (200), Iterationen $B$ (1000), Burn-in $L$(100)
    - Meist ist wenig Feinjustierung n√∂tig; BART funktioniert oft direkt gut

![Tree Perturbations](https://i.postimg.cc/D0Bk2dsP/BART.jpg)


```python=
from ISLP.bart import BART
Bart = BART(random_state=0, burnin=5, ndraw=15).fit(X_train, y_train)
np.mean((Bart.predict(X_test.astype(np.float32)) - y_test) ** 2)
Bart.variable_inclusion_.mean(0)
```

## SVM

**Hyperplane**
- Definition: Eine Hyperplane ist eine flache affine Untermannigfaltigkeit der Dimension p-1
  - In 2D: Linie
  - In 3D: Ebene
  - In p>3: Schwer vorstellbar, aber gleiche Definition
- Mathematische Definition:
  - 2D: $\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$
  - p-Dimensionen: $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p = 0$
- Positionierung:
  - $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p > 0$: Punkt liegt auf einer Seite
  - $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p < 0$: Punkt liegt auf der anderen Seite
- Klassen: $y_1, \ldots, y_n \in \{-1, 1\}$
- Klassifikation mit einer trennenden Hyperplane:
  - $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} > 0 \text{ f√ºr } y_{i} = 1$
  - $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} < 0 \text{ f√ºr } y_{i} = -1$
- Problem: Unendlich viele trennende Hyperplanes m√∂glich
- L√∂sung: Maximal Margin Hyperplane w√§hlen
  - Gr√∂√üte minimale Distanz zu den Trainingsbeobachtungen
  - Abh√§ngigkeit von den Support Vektoren
    - Beobachtungen, die die Margin beeinflussen
    - Nur Support Vektoren beeinflussen den Classifier

**Maximal Margin Classifier**
- Optimierungsproblem:
  - Maximieren von $M$
  - Einschr√§nkungen: $\sum_{j=1}^{p}\beta_j^2=1$, $y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip})\geq M$
    - $y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip})$: den Abstand der *i*-ten Beobachtung zur Hyperbene.
    - $\beta_0, \beta_1, ..., \beta_p$: Koeffizienten der maximalen Marginal-Hyperbene
    - $\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip} = 0$: Definiert die Hyperbene
    - $\sum_{j=1}^{p}\beta_j^2=1$: Stellt sicher, dass dieser Ausdruck den senkrechten Abstand der *i*-ten Beobachtung zur Hyperbene repr√§sentiert
- Wenn es keine trennende Hyperplane existiert, verwenden Soft Margin (Support Vector Classifier), um Klassen fast zu trennen

**Support Vector Classifiers**
- √úberblick: Support Vector Classifier (auch Soft Margin Classifier genannt) erlaubt einige Fehler, um die meisten Beobachtungen korrekt zu klassifizieren
- Optimierungsproblem:
  - Maximierung der Margin $M$
  - Einbeziehung von Slack-Variablen $\epsilon_i$ erlaubt Fehler
  - Die Einschr√§nkung: $y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip})\geq M(1-\epsilon_i)$
    - $\epsilon_i = 0$:
      - Beobachtung ist korrekt klassifiziert
    - $\epsilon_i > 0$:
      - Beobachtung hat den Margin verletzt
      - Kann dennoch auf der richtigen Seite der Hyperbene liegen
    - $\epsilon_i > 1$:
      - Beobachtung liegt auf der falschen Seite der Hyperbene (fehlklassifiziert)
- Parameter $C$: kontrolliert die zul√§ssige Summe der $\epsilon_i$'s, fungiert als Budget f√ºr Margin-Verletzungen
  - $C = 0$: Keine Toleranz f√ºr Fehler (maximal margin hyperplane)
  - $C > 0$: Erlaubt Fehler, gr√∂√üere Margin
  - Gr√∂√üeres $C$:
      - Mehr Toleranz, breitere Margin
      - Mehr Support Vektoren, niedrige Varianz, hoher Bias
  - Kleineres $C$:
      - Weniger Toleranz, schmalere Margin
      - Weniger Support Vektoren, hohe Varianz, niedriger Bias
- Ein Support Vector Classifier eignet sich gut f√ºr lineare Klassifikationsgrenzen
    - Bei nicht-linearen Grenzen versagt der SVC. L√∂sung: Erweiterung des Merkmalsraums durch quadratische, kubische oder h√∂here Polynome
        - Beispiel: Statt ùëù Merkmale $X_1, X_2, \ldots, X_p$ nutzen wir 2ùëù Merkmale $X_1, X_1^2, X_2, X_2^2, \ldots , X_p, X_p^2$
    - Resultat: In erweitertem Merkmalsraum ist die Grenze linear, im urspr√ºnglichen Raum jedoch nicht-linear
    - Zu viele Merkmale k√∂nnen Berechnungen unhandlich machen

```python=
from sklearn.svm import SVC
import sklearn.model_selection as skm
SVC_Model = SVC(C=1e5, kernel='linear').fit(X, y)
# H√∂her C-Wert: Modell bestraft Fehlklassifikationen stark (kleiner Margin)
# Niedriger C-Wert: Modell erlaubt mehr Fehlklassifikationen (gr√∂√üer Margin)
# linear Kernel: eine lineare Hyperebene
# rbf Kernel: eine Radial-Basis-Funktion
kfold = skm.KFold(5, random_state=0, shuffle=True)
Grid_SVC = skm.GridSearchCV(SVC_Model,{'C':[5,10,100]}, refit=True, cv=kfold, scoring='accuracy')
Grid_SVC.fit(X,y)
Grid_SVC.best_params_
```

**Support Vector Machine**
- SVM erweitert SVC durch Verwendung von Kernels
- Kernels definieren die √Ñhnlichkeit zweier Beobachtungen
    - Lineares Kernel: $K(x_i,x_{i\prime}) = \sum_{j=1}^{p}{x_{ij}x_{i\prime j}}$
    - Polynomiales Kernel: $K(x_i,x_{i\prime}) = (1+\sum_{j=1}^{p}{x_{ij}x_{i\prime j}})^d$ -> passt besser zu nicht-linearen Daten
    - Radial Kernel: $K(x_i,x_{i'})=exp(-\gamma\sum_{j=1}^{p}(x_{ij}-x_{i^\prime j})^2)$
- Kernels erm√∂glichen effiziente Berechnungen ohne expliziten Merkmalsraum 
- Die Kernelmatrix enth√§lt die paarweisen Kernel-√Ñhnlichkeiten aller Trainingsdatenpunkte
    - Beispiel: n = 3, p = 2
        - $K = \begin{bmatrix} K(x_1, x_1) & K(x_1, x_2) & K(x_1, x_3) \\ K(x_2, x_1) & K(x_2, x_2) & K(x_2, x_3) \\ K(x_3, x_1) & K(x_3, x_2) & K(x_3, x_3) \end{bmatrix}$
- Zur Klassifikation neuer Daten berechnet die SVM die Kernel-√Ñhnlichkeit zu den Support-Vektoren und weist anhand dieser Werte eine Klasse zu

```python=
from sklearn.svm import SVC
import sklearn.model_selection as skm
SVM_Model = SVC(kernel="rbf", gamma=1, C=1).fit(X, y)
# kernel="poly": degree gibt den Grad des Polynoms an
# kernel="rbf": gamma gibt den Koeffizienten des Kernels an
# Ein zu hoher C-Wert kann zu Overfitting f√ºhren
kfold = skm.KFold(5, random_state=0, shuffle=True)
Grid_SVM = skm.GridSearchCV(svm_rbf, refit=True, cv=kfold, scoring='accuracy', {'C':[0.1,10,100], 'gamma':[0.5,2,3]})
Grid_SVM.fit(X, y)
Grid_SVM.best_params_
```

**ROC Kurve**

- Klassifikatoren wie LDA und SVM berechnen f√ºr jede Beobachtung Scores
    - Form von LDA oder SVC: $\hat{f}(X) = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \ldots + \hat{\beta}_p X_p$
- Schwellenwert $t$ teilt Beobachtungen anhand des Scores $\hat{f}(X)$ in zwei Kategorien:
  - $\hat{f}(X) < t$: Kategorie 1 (z.B. "Herzkrankheit")
  - $\hat{f}(X) \geq t$: Kategorie 2 (z.B. "keine Herzkrankheit")
- ROC-Kurve: Zeigt die wahre Positivrate (y-Achse) gegen die falsche Positivrate (x-Achse) f√ºr verschiedene Schwellenwerte *t*
  - Falsche Positivrate: Anteil f√§lschlich positiver Klassifikationen
  - Wahre Positivrate: Anteil korrekt positiver Klassifikationen
    <table>
        <tr>
            <th></th>
            <th>Actual Positive</th>
            <th>Actual Negative</th>
        </tr>
        <tr>
            <th>Predicted Positive</th>
            <td>Wahre Positive (TP)</td>
            <td>Falsche Positive (FP)</td>
        </tr>
        <tr>
            <th>Predicted Negative</th>
            <td>Falsche Negative (FN)</td>
            <td>Wahre Negative (TN)</td>
        </tr>
        <tr>
            <th></th>
            <td>P</td>
            <td>N</td>
        </tr>    
    </table>
    
    - Sensitivit√§t (True Positive Rate, TPR) = TP/(TP + FN) = TP/P
    - Falsche Positive Rate (FPR) oder 1 - Specificity = FP/(FP + TN) = FP/N
    - TP/P : Sensitivity, Power, Recall, 1 - Type II Error
    - FP/N : Type I Error, 1 - Specificity
    - TP/(TP+FP) : Precision, 1 - False Discovery Proportion
- Optimaler Klassifikator liegt im oberen linken Eckpunkt der ROC-Kurve (hohe wahre, niedrige falsche Positivrate)
- H√∂here Kurve = besserer Klassifikator

```python=
from sklearn.metrics import RocCurveDisplay
roc_curve = RocCurveDisplay.from_estimator
fig, ax = plt.subplots(figsize=(8,8))
roc_curve(SVM_Model, X, y, name='Training', color='r', ax=ax)
```

**SVM with Multiple Classes**

- Ans√§tze: One-Versus-One (OvO) und One-Versus-All (OvA) or One-Versus-Rest (OvR)
- OvO:
    - Konstruktion von $\binom{K}{2}$ SVMs, jede vergleicht ein Klassenpaar
    - Zuweisung zur Klasse, die am h√§ufigsten gew√§hlt wurde
- OvA:
    - Konstruktion von $K$ SVMs, jede vergleicht eine Klasse mit den anderen $K-1$ Klassen
    - Zuweisung zur Klasse mit gr√∂√ütem Wert $\beta_{0k} + \beta_{1k} x_1^\ast + \ldots + \beta_{pk} x_p^\ast$

```python=
from sklearn.svm import SVC
OvO = SVC(kernel="rbf", C=10, gamma=1, decision_function_shape='ovo')
OvR = SVC(kernel="rbf", C=10, gamma=1, decision_function_shape='ovr')
```

## Clustering

**K-Means Clustering**

- K-Means-Clustering ist eine einfache und elegante Methode, um einen Datensatz in vorgegebene K verschiedene, nicht √ºberlappende Cluster zu unterteilen.
    - Die Auswahl von K ist ein nicht-triviales Problem.
    - Jede Beobachtung geh√∂rt zu mindestens einem Cluster.
- Die Variation innerhalb eines Clusters $W(C_k)$ misst, wie sehr sich die Beobachtungen innerhalb eines Clusters voneinander unterscheiden.
    - $W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k} \sum_{j=1}^{p} (x_{ij} - x_{i'j})^2$
        - $|C_k|$ = Anzahl der Beobachtungen im $k$-ten Cluster
- Eine gute Clusterbildung minimiert die Variation innerhalb der Cluster $\text{minimize}_{C_1, \ldots, C_K} \left\{ \sum_{k=1}^{K} W(C_k) \right\}$
- Algorithmus:
  - Problem: Die Anzahl m√∂glicher Partitionen ist nahezu $K^n$, was eine exakte L√∂sung au√üer bei kleinen K und n schwierig macht.
  - Ein einfacher Algorithmus findet ein lokales Optimum (eine ziemlich gute L√∂sung):
    - Schritt 1: Weise jeder Beobachtung zuf√§llig eine Zahl von 1 bis K zu (initiale Clusterzuweisung).
    - Schritt 2: Iteriere, bis sich die Clusterzuweisungen nicht mehr √§ndern:
      - (a) F√ºr jedes der K Cluster berechne das Cluster-Zentrum:
        - Das Zentrum des $k$-ten Clusters ist der Vektor der Mittelwerte der p Merkmale der Beobachtungen in Cluster $k$.
      - (b) Weise jede Beobachtung dem Cluster zu, dessen Zentrum am n√§chsten liegt (unter Verwendung der euklidischen Distanz).
- Der K-Means-Algorithmus findet ein lokales, aber kein globales Optimum.
- Die Ergebnisse h√§ngen von der initialen (zuf√§lligen) Clusterzuweisung in Schritt 1 ab.
    - F√ºhre den Algorithmus mehrmals mit unterschiedlichen zuf√§lligen Anfangskonfigurationen aus.
    - W√§hle die beste L√∂sung (kleinster Wert der Variation innerhalb der Cluster).

```python=
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2, random_state=2, n_init=20).fit(X)
kmeans.labels_
```

**Hierarchical Clustering**

- Die h√§ufigste Art des hierarchischen Clusterings ist das Bottom-up- oder agglomerative Clustering.
- Dendrogramm-Struktur:
    - Jedes Blatt stellt eine der Beobachtungen dar.
    - Beim Aufsteigen im Baum:
        - Bl√§tter beginnen, sich zu √Ñsten zu verbinden, was auf √§hnliche Beobachtungen hinweist.
        - √Ñste selbst verbinden sich auf h√∂heren Ebenen mit anderen Bl√§ttern oder √Ñsten.
    - Fr√ºhere (niedrigere) Fusionen deuten auf eine h√∂here √Ñhnlichkeit zwischen Beobachtungen/Gruppen hin.
    - Sp√§tere (h√∂here) Fusionen deuten auf gr√∂√üere Un√§hnlichkeit hin.
    - Die H√∂he der ersten Fusion (auf der vertikalen Achse) zwischen zwei Beobachtungen misst deren Un√§hnlichkeit.
    - Die √Ñhnlichkeit wird durch die vertikale Fusionsh√∂he bestimmt, nicht durch die horizontale N√§he.
    - Um Cluster mit einem Dendrogramm zu identifizieren:
        - Ziehe einen waagerechten Schnitt durch das Dendrogramm.
        - Die Beobachtungsgruppen unterhalb des Schnitts sind die Cluster.
        - Ein Dendrogramm kann beliebig viele Cluster liefern.
        - Praktiker w√§hlen die Anzahl der Cluster oft durch visuelle Inspektion des Dendrogramms.
- F√ºr manche Datens√§tze spiegelt diese hierarchische Struktur m√∂glicherweise nicht die tats√§chliche Gruppierung wider.
    - Beispiel: Wenn Beobachtungen M√§nner und Frauen sind, aufgeteilt in Amerikaner, Japaner und Franzosen:
        - Beste Aufteilung in zwei: nach Geschlecht.
        - Beste Aufteilung in drei: nach Nationalit√§t.
        - Die wahren Cluster (nach Nationalit√§t) sind nicht innerhalb der geschlechtsbasierten Cluster verschachtelt.
    - Hierarchisches Clustering kann nicht immer nicht-verschachtelte Cluster darstellen und liefert m√∂glicherweise weniger genaue Ergebnisse als K-Means f√ºr eine gegebene Clusteranzahl.
- Algorithmus:
  - Initialisierung
    - Beginne mit der Definition eines Un√§hnlichkeitsma√ües zwischen jedem Beobachtungspaar.
      - Am h√§ufigsten verwendet: euklidische Distanz.
      - Die Wahl des Un√§hnlichkeitsma√ües kann variieren (sp√§ter im Kapitel diskutiert).
  - Iterativer Clustering-Prozess
    - Beginne am unteren Rand des Dendrogramms:
      - Jede der $n$ Beobachtungen ist zu Beginn ihr eigenes Cluster.
    - In jedem Schritt:
      - Identifiziere die beiden √§hnlichsten (am wenigsten un√§hnlichen) Cluster.
      - Verbinde diese beiden Cluster miteinander, wodurch die Clusteranzahl um eins reduziert wird.
      - Wiederhole dies, bis alle Beobachtungen zu einem einzigen Cluster geh√∂ren.
    - Endergebnis: Ein vollst√§ndiges Dendrogramm, das das hierarchische Clustering darstellt.
- Un√§hnlichkeit zwischen Clustern
    - Wie definiert man die Un√§hnlichkeit zwischen zwei Clustern, wenn diese mehrere Beobachtungen beinhalten k√∂nnen?
    - L√∂sung: Nutze das Konzept der Linkage, um die Un√§hnlichkeit zwischen Clustern zu definieren.
    - Complete Linkage: Berechne alle paarweisen Un√§hnlichkeiten zwischen Beobachtungen in den Clustern A und B; notiere den gr√∂√üten Wert. F√ºhrt zu balancierten Dendrogrammen.
    - Single Linkage: Notiere den kleinsten Wert. Kann zu verl√§ngerten, "schl√§ngelnden" Clustern mit Einzel-Fusionen f√ºhren.
    - Average Linkage: Notiere den Durchschnittswert. Wird von Statistikern bevorzugt und liefert balancierte Dendrogramme.
    - Centroid Linkage: Un√§hnlichkeit zwischen den Schwerpunkten (Mittelwert-Vektoren) der Cluster A und B. H√§ufig in der Genomik. Kann zu Inversionen f√ºhren (Cluster werden auf einer H√∂he verschmolzen, die unter der einzelner Cluster liegt), was Visualisierungs- und Interpretationsprobleme verursachen kann.
    - Average, Complete und Single Linkage sind unter Statistikern am beliebtesten; Average und Complete werden im Allgemeinen gegen√ºber Single bevorzugt.
- Wahl des Un√§hnlichkeitsma√ües
    - Euklidische Distanz: Wird als Standard-Un√§hnlichkeitsma√ü verwendet.
    - Korrelationsbasierte Distanz:
        - Betrachtet zwei Beobachtungen als √§hnlich, wenn ihre Merkmale hoch korreliert sind.
        - Konzentriert sich auf die Form der Beobachtungsprofile, nicht auf deren Gr√∂√üe.
    - Hat starken Einfluss auf das entstehende Dendrogramm und das Clustering-Ergebnis.
    - Die Wahl sollte sich richten nach:
        - Der Art der zu clusternden Daten.
        - Der wissenschaftlichen Fragestellung oder dem Gesch√§ftsziel.
    - Beispiel: Ein Online-H√§ndler m√∂chte K√§ufer nach Kaufhistorie clustern.
        - Daten: Zeilen = K√§ufer, Spalten = Artikel, Eintr√§ge = Kaufanzahlen.
        - Euklidische Distanz: Gruppiert seltene K√§ufer zusammen, ignoriert Pr√§ferenzen.
        - Korrelationsbasierte Distanz: Gruppiert K√§ufer mit √§hnlichen Pr√§ferenzen (z.B. kaufen A & B, nie C & D), unabh√§ngig vom Kaufvolumen ‚Äì besser geeignet, um pr√§ferenzbasierte Subgruppen zu finden.
- Variablen skalieren
    - Einige Artikel werden h√§ufiger gekauft (z.B. Socken vs. Computer), wodurch h√§ufig gekaufte Artikel die Distanzma√üe dominieren k√∂nnen.
        - Beispiel: Socken-K√§ufe √ºberwiegen Computer-K√§ufe beim Clustering, was m√∂glicherweise nicht den Gesch√§ftspriorit√§ten entspricht.
    - Skalierung (Standardabweichung = 1) gibt jeder Variablen das gleiche Gewicht, verhindert Verzerrungen durch unterschiedliche Ma√üeinheiten und ist hilfreich, wenn Variablen unterschiedliche Skalen haben.
    - Das Skalieren h√§ngt von den Zielen ab und beeinflusst sowohl hierarchisches als auch K-Means-Clustering.
- Probleme
    - Kleine Entscheidungen mit gro√üen Konsequenzen: K, Linkage-Typ, Un√§hnlichkeitsma√ü.
        - Oft werden mehrere Kombinationen ausprobiert.
        - Es gibt selten eine einzig ‚Äûrichtige‚Äú L√∂sung ‚Äì jede Variante, die interessante Aspekte der Daten sichtbar macht, ist wertvoll.
    - K-Means und hierarchisches Clustering ordnen alle Beobachtungen Clustern zu, was problematisch ist, wenn es viele Ausrei√üer gibt oder die Mehrheit der Daten nur wenigen Untergruppen entspricht. Das Erzwingen von Clustern f√ºr alle Daten kann zu Verzerrungen f√ºhren.
        - Mischungsmodelle (‚Äûmixture models‚Äú) bieten ‚Äûweiches‚Äú Clustering und gehen besser mit Ausrei√üern um.
    - Clustering-Verfahren sind oft instabil; das Entfernen einiger Datenpunkte und erneutes Clustern kann sehr unterschiedliche Ergebnisse liefern. Idealerweise sollten Cluster stabil sein, sind es aber h√§ufig nicht.
        - √úberpr√ºfe die Robustheit der Cluster, indem du auf Teilmengen der Daten clustert.
    - Clustering bildet immer Gruppen, aber es ist unklar, ob diese echt sind oder nur Rauschen.
        - W√ºrden neue Daten die gleichen Cluster ergeben?
        - Dies zu beurteilen ist schwierig; einige Methoden weisen Clustern p-Werte zu, aber es gibt keinen allgemein anerkannten Ansatz.
    - L√∂sungen: F√ºhre das Clustering mit verschiedenen Parametereinstellungen durch (Standardisierung, Linkage, Clusteranzahl etc.).

```python=
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scale = scaler.fit_transform(X)
from sklearn.cluster import AgglomerativeClustering
HClust = AgglomerativeClustering
hc_comp = HClust(distance_threshold=0, n_clusters=None, linkage='complete').fit(X_scale )
# linkage='average', linkage='single'
from ISLP.cluster import compute_linkage
linkage_comp = compute_linkage(hc_comp)
from scipy.cluster.hierarchy import dendrogram, cut_tree
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp, ax=ax, color_threshold=-np.inf, above_threshold_color='k')
cut_tree(linkage_comp, n_clusters=4).T
```

```python=
corD = 1 - np.corrcoef(X_scale)
from sklearn.cluster import AgglomerativeClustering
HClust = AgglomerativeClustering
hc_cor = HClust(linkage='complete', distance_threshold=0, n_clusters=None, metric='precomputed').fit(corD)
# metric='precomputed' indicates that the input data is a precomputed distance matrix.
# metric='precomputed' for Correlation-based distance
```
